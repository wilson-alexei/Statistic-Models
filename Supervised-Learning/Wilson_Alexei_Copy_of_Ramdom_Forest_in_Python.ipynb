{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz0tWZ69EZ1z"
      },
      "source": [
        "# Understanding Random Forests Classifiers in Python\n",
        "Learn about Random Forests and build your own model in Python, for both classification and regression.\n",
        "Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n",
        "\n",
        "Random forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.\n",
        "\n",
        "In this tutorial, you are going to learn about all of the following:\n",
        "- The random forests algorithm\n",
        "- How does the classifier work?\n",
        "- Its advantages and disadvantages\n",
        "- Finding important features\n",
        "- Comparision between random forests and decision trees\n",
        "- Building a classifier with scikit-learn\n",
        "- Finding important features with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIkgDjKEZ15"
      },
      "source": [
        "## The Random Forests Algorithm\n",
        "Let’s understand the algorithm in layman’s terms. Suppose you want to go on a trip and you would like to travel to a place which you will enjoy.\n",
        "\n",
        "So what do you do to find a place that you will like? You can search online, read reviews on travel blogs and portals, or you can also ask your friends.\n",
        "\n",
        "Let’s suppose you have decided to ask your friends, and talked with them about their past travel experience to various places. You will get some recommendations from every friend. Now you have to make a list of those recommended places. Then, you ask them to vote (or select one best place for the trip) from the list of recommended places you made. The place with the highest number of votes will be your final choice for the trip.\n",
        "\n",
        "In the above decision process, there are two parts. \n",
        "\n",
        "- First, asking your friends about their **individual travel experience** and **getting one recommendation** out of multiple places they have visited. This part is like using the decision tree algorithm. Here, each friend makes a selection of the places he or she has visited so far.\n",
        "\n",
        "- The second part, after collecting all the recommendations, is the **voting procedure** for **selecting the best place** in the list of recommendations. This whole process of getting recommendations from friends and voting on them to find the best place is known as the **random forests algorithm**.\n",
        "\n",
        "It technically is an **ensemble** method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as **information gain**, **gain ratio**, and **Gini index** for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result. In the case of regression, the average of all the tree outputs is considered as the final result. It is simpler and more powerful compared to the other non-linear classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzfYy0gjEZ16"
      },
      "source": [
        "## How does the classifier work?\n",
        "It works in four steps:\n",
        "\n",
        "1. Select random samples from a given dataset.\n",
        "2. Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
        "3. Perform a vote for each predicted result.\n",
        "4. Select the prediction result with the most votes as the final prediction.\n",
        "\n",
        "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT71WTNlEZ17"
      },
      "source": [
        "## Advantages:\n",
        "- Random forests is considered as a highly **accurate** and **robust** method because of the number of decision trees participating in the process.\n",
        "- It does not suffer from the **overfitting** problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
        "- The algorithm can be used in both **classification** and **regression** problems.\n",
        "- Random forests can also handle **missing values**. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
        "- You can get the relative feature importance, which helps in **selecting the most contributing features** for the classifier.\n",
        "\n",
        "## Disadvantages:\n",
        "- Random forests is **slow** in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
        "- The model is **difficult to interpret** compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
        "\n",
        "__NOTE__: In data science, we use RF a lot since we do not care as much as interpretation of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPZiPbdaEZ18"
      },
      "source": [
        "## Finding important features\n",
        "Random forests also offers a good **feature selection** indicator. `Scikit-learn` provides an extra variable with the model, which shows the relative importance or contribution of each feature in the prediction. It automatically computes the `relevance score` of each feature in the training phase. Then it scales the relevance down so that the sum of all scores is `1`. __Higher relevance score means more important feature__.\n",
        "\n",
        "This score will help you choose the most important features and drop the least important ones for model building.\n",
        "\n",
        "Random forest uses `gini importance` or `mean decrease in impurity` (MDI) to calculate the importance of each feature. Gini importance is also known as the total decrease in node impurity. This is how much the model fit or accuracy decreases when you drop a variable. The larger the decrease, the more significant the variable is. Here, the mean decrease is a significant parameter for variable selection. The Gini index can describe the **overall explanatory power** of the variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vG4ePDEZ19"
      },
      "source": [
        "## Random Forests vs Decision Trees\n",
        "- Random forests is a set of multiple decision trees.\n",
        "- Deep decision trees may suffer from overfitting, but random forests prevents overfitting by creating trees on random subsets.\n",
        "- Decision trees are computationally faster.\n",
        "- Random forests is difficult to interpret, while a decision tree is easily interpretable and can be converted to rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIw8IjwKEZ19"
      },
      "source": [
        "## Building a Classifier using Scikit-learn\n",
        "You will be building a model on the `iris flower dataset`, which is a very famous classification set. It comprises the `sepal length`, `sepal width`, `petal length`, `petal width`, and `type of flowers`. There are three species or classes: setosa, versicolor, and virginia. You will build a model to classify the type of flower. The dataset is available in the scikit-learn library or you can download it from the UCI Machine Learning Repository.\n",
        "\n",
        "Start by importing the datasets library from scikit-learn, and load the iris dataset with `load_iris()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Fd12xWEZ1-"
      },
      "source": [
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbKgi9CaEZ2D"
      },
      "source": [
        "You can print the target and feature names, to make sure you have the right dataset, as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBd__BeWEZ2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9a9de8-53d9-4f39-a417-519f793a722d"
      },
      "source": [
        "# print the label species(setosa, versicolor,virginica)\n",
        "print(iris.target_names)\n",
        "\n",
        "# print the names of the four features\n",
        "print(iris.feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['setosa' 'versicolor' 'virginica']\n",
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSzeXgCIEZ2H"
      },
      "source": [
        "It's a good idea to always explore your data a bit, so you know what you're working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drp8Bm1gEZ2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e854a937-b89d-4c62-d924-11a5e84b9941"
      },
      "source": [
        "# print the iris data (top 5 records)\n",
        "print(iris.data[0:5])\n",
        "\n",
        "# print the iris labels (0:setosa, 1:versicolor, 2:virginica)\n",
        "print(iris.target[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "[0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpxwLxzEZ2M"
      },
      "source": [
        "Here, you can create a `pandas` DataFrame of the iris dataset the following way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6X-4a3kEZ2N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6e194c0f-99f3-41a9-fc8e-4ed9cb4d4102"
      },
      "source": [
        "# Creating a DataFrame of given iris dataset.\n",
        "import pandas as pd\n",
        "data = pd.DataFrame({\n",
        "    'sepal length':iris.data[:,0],\n",
        "    'sepal width':iris.data[:,1],\n",
        "    'petal length':iris.data[:,2],\n",
        "    'petal width':iris.data[:,3],\n",
        "    'species':iris.target\n",
        "})\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length</th>\n",
              "      <th>sepal width</th>\n",
              "      <th>petal length</th>\n",
              "      <th>petal width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length  sepal width  petal length  petal width  species\n",
              "0           5.1          3.5           1.4          0.2        0\n",
              "1           4.9          3.0           1.4          0.2        0\n",
              "2           4.7          3.2           1.3          0.2        0\n",
              "3           4.6          3.1           1.5          0.2        0\n",
              "4           5.0          3.6           1.4          0.2        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SIicVkeEZ2Q"
      },
      "source": [
        "Now we can start building our classifier. First, you separate the columns into predictor and target variables (or features and labels). Then you split those variables into a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yHdEeE4EZ2Q"
      },
      "source": [
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
        "y=data['species']  # Labels\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2019) # 70% training and 30% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ZUy0aFEZ2U"
      },
      "source": [
        "After splitting, you will train the model on the training set and perform predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYa-mbwKEZ2V"
      },
      "source": [
        "#Import Random Forest Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGq4oknmEZ2a"
      },
      "source": [
        "After training, check the predictive performance using actual and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvwy4AV7EZ2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab7d7ea-46f2-4855-ae7b-d3b7b1ed3cee"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.90      0.95        10\n",
            "           2       0.94      1.00      0.97        16\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hqK8w5iEZ2d"
      },
      "source": [
        "You can see that our model is well trained at a f1-score of `98%`.\n",
        "\n",
        "Let's try applying the trained model on a totally unseen item, for example:\n",
        "\n",
        "- sepal length = 3\n",
        "- sepal width = 5\n",
        "- petal length = 4\n",
        "- petal width = 2\n",
        "\n",
        "__NOTE__: this is called model deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjz-Y1s8EZ2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ca59e6-a6c8-4634-fb8a-a2ee0189568f"
      },
      "source": [
        "clf.predict([[3, 5, 4, 2]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S68rlGVcEZ2h"
      },
      "source": [
        "Here, 2 indicates the flower type Virginica.\n",
        "\n",
        "\n",
        "## Finding Important Features in Scikit-learn\n",
        "Here, you are finding important features or selecting features in the IRIS dataset. In scikit-learn, you can perform this task in the following steps:\n",
        "\n",
        "- First, you need to create a random forests model.\n",
        "- Second, use the feature importance variable to see feature importance scores.\n",
        "- Third, visualize these scores using the seaborn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfUmK3sEZ2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80504325-56a6-4212-f475-eec57b689d7c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emgJr1K4EZ2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d2c211-b0d4-42d2-d4ee-c6621bb8d197"
      },
      "source": [
        "import pandas as pd\n",
        "feature_imp = pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)\n",
        "feature_imp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "petal width (cm)     0.451554\n",
              "petal length (cm)    0.436185\n",
              "sepal length (cm)    0.071550\n",
              "sepal width (cm)     0.040712\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umR9fo87EZ2o"
      },
      "source": [
        "You can also visualize the feature importance. Visualizations are **easy to understand and interpretable**.\n",
        "\n",
        "For visualization, you can use a combination of `matplotlib` and `seaborn`. Because `seaborn` is built on top of `matplotlib`, it offers a number of customized themes and provides additional plot types. `Matplotlib` is a superset of `seaborn` and both are equally important for good visualizations.\n",
        "\n",
        "__NOTE__: Checking and displaying the feature importances, regardless of applying feature engineering, are always good practices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsAcIBM4EZ2o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "62257cda-f093-4997-cb72-0caa70ed4c40"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "# Creating a bar plot\n",
        "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
        "# Add labels to your graph\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title(\"Visualizing Important Features\")\n",
        "#plt.legend(iris.feature_names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STwNu63-EZ2s"
      },
      "source": [
        "## Generating the Model on Selected Features\n",
        "Here, you can remove the \"sepal width\" feature because it has very low importance, and select the 3 remaining features.\n",
        "\n",
        "__NOTE__: this is a feature selection step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy18Nh1pEZ2t"
      },
      "source": [
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split dataset into features and labels\n",
        "X=data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\"\n",
        "y=data['species']                                       \n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=2021) # 70% training and 30% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9szwI9JEZ2x"
      },
      "source": [
        "After spliting, you will generate a model on the selected training set features, perform predictions on the selected test set features, and compare actual and predicted values. \n",
        "\n",
        "__NOTE__: We use the same `ramdom_state` so that we can compare model performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seMNImBNEZ2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778cc0b9-9ff5-4d49-8c67-82a372d64386"
      },
      "source": [
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "# prediction on test set\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "#from sklearn import metrics\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.90      0.95        10\n",
            "           2       0.94      1.00      0.97        16\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0NFTaE0EZ21"
      },
      "source": [
        "You can see that after removing the least important features (sepal length), the accuracy did not decrease. This is because you removed misleading data and noise, resulting in a comparable performance. The main value in this step is that a lesser amount of features also reduces the training time.\n",
        "\n",
        "## Conclusion\n",
        "Congratulations, you have made it to the end of this tutorial!\n",
        "\n",
        "In this tutorial, you have learned what random forests is, how it works, finding important features, the comparison between random forests and decision trees, advantages and disadvantages. You have also learned model building, evaluation and finding important features in `scikit-learn`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1fdzqy5EZ21"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}